\chapter{Historical Background on Free and Open Source Software}
\label{historical_outline}

The software is the logic part of a computer system, while the hardware is the physical part. Loosely speaking we could say that computer programs are written in different programming languages, the set of instructions forming a program is called source code. This source code is compiled in order to translate it to a language that can be executed by the hardware; the result is the binary code ---a sequence of 0 and 1---. Based only in the binary code of a program no human can understand how the program develops the task for which it was designed. It is like a black box that receives some \emph{inputs} and returns some \emph{outputs}.

There are two types of software programs according to their function within a computer system: application software are programs that develop specific tasks useful to the user, such as a word processor or a web browser. System software are programs that conform the operating system, according to Tanenbaum and Woodhull the two main functions of an operating system are, on the one hand, being an abstraction layer that provides to applications --- and to developers who write applications--- a set of simple operations that hide the complexity of hardware. On the other hand, the operating system is responsible for managing system resources ---RAM, processor, disk space ,...--- between different programs competing for them when they run in the system \citep[3-5]{tanenbaum:1996}. From a more practical but less rigorous point of view, we can conceive an operating system as the minimal set of programs that allow a computer to do useful things. Because what is considered useful has evolved along time, operating systems have also evolved.

At the beginnings of computer science, there was no clear distinction between software and hardware and between developer and user. There were only people who gave precise instructions to computers about what they should do. In 1952 IBM commercialized the first computer and during this decade began to spread their use \citep[21]{weber:2004}. One of the key events that marked the evolution of software at this time was the decision of the US Department of Justice that Western Electric and American Telephone and Telegraph (AT\&T) could not join to work together beyond the field telecommunications. This decision, taken in 1956 under anti-trust legislation, lead AT\&T to promote software licenses to a nominal cost and to release in the public domain the output of research developed at Bell Labs, in order to not violate anti-trust laws \citep[20]{roca:2007}.

\section{UNIX and the C language}

In the 1960s, computers were very expensive facilities that were only available to government centers, some large companies, and universities. The study, design, and implementation of operating systems largely focused research efforts in those years. A major project was the result of collaboration between MIT researchers and staff of Bell Labs, the goal was to build an operating system called MULTICS (\emph{Multiplexed Information and Computing Service}. The results of those efforts did not success; in 1969 AT\&T withdrew from the project. But two researchers who had participated in the work on MULTICS, Ken Thompson and Dennis Ritchie, developed on their own a new operating system, based in part on their work in MULTICS, which was called UNIX. The first implementation was made entirely by Thompson in a month during the summer of 1969 and consisted in a \emph{kernel}, a shell, a text editor and an assembly language \citep[26]{weber:2004}.

Assembly languages are tightly linked to each hardware platform, that is, each kind of computer has its own assembly language, which is incompatible with other assembly languages. Thus it was not possible to run software written for a computer in any other computer. In the early 1970s, Dennis Ritchie invented C, a general-purpose programming language, which allowed to write the source code of software once, and was able to run in a wide range of hardware thanks to a compiler, that translated the common C source code to the particular hardware instructions for each kind of computer. Until the early 1980s, although compilers existed for a variety of hardware, the C language was almost exclusively associated with UNIX; more recently, its use has spread much more widely, and today it is among the programming languages most commonly used \citep{ritchie:1993}.

The first impulse to the spread of UNIX operating system was a computer science symposium in which Thompson and Ritchie presented a paper about UNIX. The authors offered to send a copy of UNIX to whom were interested, petitions exceeded by far initial expectations of the authors. The interest grew when they rewrote UNIX with the C programming language, so it could work on any hardware that had a C compiler. Given this growing interest, AT\&T ---constrained by anti-trust legislation--- decided to license UNIX, at first, under minimal conditions: the software was provided without warranty (\emph{`as-is'}) and without support or correction of errors by the company; paying a fee of several hundred of dollars AT\&T sent a copy of the source code of UNIX \citep[28-29]{weber:2004}.

The UNIX operating System became popular as a teaching and research tool in computer science departments in universities around the world, especially at US. The availability of source code enabled experimentation, modification and improvement of the UNIX system. AT\&T did not offer support or maintenance, so system users had a strong incentive to share solutions to bugs and improvements with the user community. Users of computers at that time were not like today, they were in large part, college students, scientists or engineers with extensive technical training. UNIX was one of the first experiences of collaboration and knowledge exchange on a large scale in the field of software.

One of the main actors in the development of UNIX was the University of Berkeley, which began its own distribution of UNIX called BSD (\emph{Berkeley Software Distribution}) in the 1970s. This distribution was started based on the UNIX source code of the company AT\&T, but they added significant improvements. In 1976 Thompson joined the Berkeley team. The authorship of the contributions to the source code were collected in the same source, following the practice established by Thompson and Ritchie \citep[27]{weber:2004}. In 1983 Berkeley's team published the 4.2 version of BSD UNIX which had major improvements, the most notable of which was an implementation of the TCP/IP stack ---the communication protocol of Internet---, this version of BSD UNIX is one of the foundations of the Internet as we know it today \citep[35]{weber:2004}. The 4.2 version of BSD UNIX directly competed with a version of the company AT\&T but it was far superior technically. The liberal license terms of BSD UNIX allow to build proprietary implementations on top of BSD UNIX, this allowed the emergence of new companies that commercialized modified versions of UNIX. 

Requests for licenses from UNIX in the late seventies and early eighties increased considerably, mainly from large companies, military institutions, universities, and research centers. AT\&T and Bell Labs had to be separated by court order in 1984. As a result, Bell Labs began trading for the price of hundreds of thousands of dollars for new licenses of UNIX, which restricted drastically the number of institutions that could afford it. They also began a series of lawsuits in order to prevent the free dissemination of the various implementations of UNIX, particularly the implementation of Berkeley \cite[22]{roca:2007}. The dynamics of litigation lasted until the 1990s. This fact jeopardized the development of UNIX BSD; the future legal viability of the system was not clear. This uncertainty prompted the emergence of alternatives that, in its infancy, were technically inferior.

\section{GNU and Linux}

Richard Stallman started working in the Laboratory of Artificial Intelligence (AI) of MIT in 1971. In his own words, he joined a community that have shared the software for many years. Stallman says poetically that the act of sharing software is as old as computers, just as sharing recipes is as old as cooking \citep{stallman:1998}. At that time, the source code was accessible to all users and the act of sharing modifications involving improvements with the rest of the community was the norm. According to Stallman, this situation changed in the early eighties of the twentieth century when the community of MIT hackers collapsed. One spin-off of the MIT AI lab hired almost all the people working there. The contract contained a non-disclosure agreement forcing people to not disclose their work and therefore prevented them to publish or share their work. In addition, in 1982, the MIT AI lab changed its hardware and a proprietary operating system were installed on them.

According to Stallman, those events led him to abandon his work at the MIT AI lab because, on the one hand, ethically he could not continue working with proprietary software and, on the other hand, the community in which he worked was dismantled. But rather than stop using software and engage in other activities, he decided to promote the construction of a new community within which they could restore the practice of sharing software. Stallman explained that he thought that the first step to restore the community was building a free operating system, because it is the essential tool in order to make a computer work. Thus was born the GNU project (\emph{GNU is Not UNIX}) which aimed to create a general-purpose operating system that was a completely free reimplementation of UNIX  \citep{stallman:1985}.

Stallman devise a set of formal rules, which revolve around the concept of copyleft. This new concept is supported in the legislation about copyright. In Stallman's words, the idea of copyleft is that the author of a program gives everyone, without exception, permission to execute, copy, modify and distribute modified versions of the program. Stallman argues that in order to be effective, copyleft requires that derivative works of a program must also be free \citep{stallman:1998}. In this way privatization by software companies can be avoided, unlike free licenses that are permissive with private ownership, as the BSD license which allows a company to make changes to a free program and commercialize it in a binary format without providing any changes in source code form. The concrete implementation of these formal rules is the GNU/GPL License.

To articulate the process of building this new free operating system, Stallman founded in 1985 the Free Software Foundation (FSF), a nonprofit foundation with the objective of supporting the free software movement and give them legal cover. A relatively small group of people joined the efforts of Stallman, which was, in part, responsible for strategic planning in the early years of GNU. Hackers of the FSF created many free programs, some of them proved to be the best in their field. In the early nineties, the GNU project had a wide range of software but lacked a kernel ---the program that interacts directly with the hardware--- to have a complete operating system.

In this context, Andrew Tanenbaum created the first version of Minix in 1987. Minix is an operating system written from scratch by Tanenbaum and their students. The main objective was to allow his students to learn by analyzing how it is made and how it works an actual operating system. The Minix's source code is supplied as part of \citet{tanenbaum:1996} book on operating systems. Linus Torvalds was a student at the University of Helsinki when he developed the first version of the Linux kernel. His aim was to write a new implementation of Minix for the popular and cheap i386 computer architecture. One of the key factors for the success of Linux was that Linux Torvalds decided to license it under the GNU/GPL license because the tools he used to develop Linux came from the GNU project. He released the source code on-line and asked everyone who wanted to collaborate with the project to submit improvement proposals.

Eric Raymond, a hacker from the old school, exposed the Linux development model in a work that has had a significant influence in the field of software development: The Cathedral \& the Bazaar \citep{raymond:1999}. He starts by explaining the perplexity he felt when he became interested in Linux. Since the mid 80's had worked with the FSF by writing free software and always followed a development model that, metaphorically, can be compared with building a cathedral. A small group of architects design the program, implement it and test it for a long time. When it successfully pass all the tests it is released. He was surprised both with Linux and its development model, which consisted in releasing the program very often, even if it had known error that had not been solved. The main idea is to rely on all the people who devote their free time to test and improve Linux, collect all the proposals, and implement the best.

Raymond qualifies metaphorically this production model as a bazaar, where anyone can contribute code to the project and each project is responsible for integrating the proposals that seem useful to the source code of the program. It is fair to say that further studies on the actual dynamics of larger projects and relevant software ---like Apache and Mozilla--- have shown that the cathedral model is not entirely abandoned, rather the actual model is an hybrid; a combination of the two models where a significant portion of the program development is provided by a relatively small group of people, but there are an extensive variety of people, who more or less sporadically, contribute to the project \citep{mockus:2002}.

We must consider that in the 1990s starts a massive deployment of Internet in some countries. Internet is the infrastructure that makes possible to weave a network of peer collaboration that characterize this production model. This development model was not invented by Linus Torvalds, is not difficult to recognize the principle of peer review of scientific practice in it. In fact other software projects, such as BSD UNIX, had adopted a model that closely resembles the practices of scientific communities. It must be said that it was quite difficult that a contribution that came from an outsider of the Berkeley team was accepted in BSD UNIX; in this sense, the classical model of development of UNIX established at Berkeley was more elitist than the Linux model, which was more open and transparent but less rigorous. The key to understand the success of the Linux model is that it was contemporary to the spread of access to global digital networks in some countries.

In the early nineties, the GNU system was almost complete, just lacked the kernel; the gap was important because the kernel is the software that allows the system to operate autonomously on the hardware. Linux filled the void that was missing, the sum of the Linux kernel and GNU applications resulted in a general purpose operating system completely free: the GNU/Linux system. But the fact that all the pieces of the operating system were available did not meant that putting them to work together was an easy thing. In the first half of the nineties, installing a GNU/Linux system required a great deal of expertise and considerable time to devote to it. In this context appear and develop different distributions of the GNU/Linux operating system, among which is the Debian project, which is the subject of empirical analysis of this research.

\section{The Debian Project}

In the early nineties of the twentieth century the most powerful free operating system was BSD UNIX. But as I said, the litigation that was submitted by the companies who hold the copyrights of UNIX threatened its future viability. This led to the emergence of alternatives, although initially were technically inferior, were substantially improved in the late nineties and early twenty-first century. These alternatives were the GNU project and the Linux kernel, the combination of which allowed to build a completely free general purpose operating system. But combine these pieces of software was not, nor is, a trivial task. Linux was in its early stages of development, many people made contributions to the source code and new versions of Linux were released on a daily basis. Therefore, it was required a great effort to have GNU/Linux system running, and even more, keep them updated. Especially for people who wanted to work \emph{with} the GNU/Linux system to develop different tasks and not \emph{in} the system \citep[30]{krafft:2005}.

In 1993 Ian Murdock, a student at Purdue University at Indiana took the initiative of creating the Debian Project\footnote{The name Debian is the contraction of the names Debra ---Murdock's wife--- and Ian.} with the goal of building a distribution of GNU/Linux system. The Debian Project's initial proposals were included in the \emph{Debian manifesto} \citep{murdock:1994}. Two of the main features of the Debian project are listed in this manifesto. First, define a new type of distribution of GNU/Linux, instead of being developed by a person or a closed group, the aim was to develop the system following an open and decentralized model inspired by Linux. Secondly, Debian was defined as a non-commercial project and focused on technical excellence instead of economic profits, but without sacrificing the aim to compete in excellence with commercial options, whether free or proprietary.

In the Debian manifesto Murdock notes that distributions are essential for the future of GNU/Linux systems, because they eliminate the need for the user to search, download, compile, install and integrate a large number of programs that are the basic components of a functional system. Murdock notes that despite the importance of the distributions, they have not received much attention by free software developers. To maintain a well integrated, error-free, and reasonably updated distribution of GNU/Linux system is not an easy nor glamorous task, it requires a great amount of work and coordination to manage complexity. Many distributions of GNU/Linux system at the time ---the most popular of which was Softlanding Linux System (SLS)--- started with a technically acceptable level, but as time passed were degenerating because they did not solve the problems that arise nor updated versions of programs distributed. Thus, it was relatively easy to start a distribution of GNU/Linux system but it was very difficult to keep it operational and functional for significant periods of time.

The Debian project, thus, does not produce all the software that distributes; their main task is software integration. The source code of the programs that composes the Debian operating system is published originally under some kind of free license by authors that typically aren't involved in the project. The aim of Debian is to integrate useful programs and package them so that an average user ---without deep knowledge of software engineering--- can install or upgrade many programs in an easy and automated way. One of the main features of the production process of Debian is modularity, that is, dividing the project into semi-independent modules, designed to work together but that can be developed relatively independently. This feature allows people with different expertise, skills, and motivation to participate in the development of the system at different levels and with different intensity.

The availability of data derived of the open nature of the project and the recent interest by community forms of organizing have triggered an interesting stream of research about the Debian project in the last years \citep{omahony:2003,coleman:2005,omahony:2007,ferraro:2010}. Those research efforts have focused mainly in the governance system, the membership process, and the ethical motivations of developers. Thus we have a good understanding of the political and individual dynamics of the project but we lack a detailed analysis of its production related dynamics. My aim is to fill this gap providing a longitudinal analysis of the global patterns of relations among developers in the production process. This allows to illustrate the relevance of a structural approach in order to understand the actual production process of the Debian operating system.

\section{The Python Language}

In the late 1980s, Guido van Rossum ---a dutch computer scientist working at the \emph{Centrum Wiskunde \& Informatica}\footnote{Which translates in English to National Research Institute for Mathematics and Computer Science}--- invented the Python programming language. A commonly cited account of the invention of Python by his author is the foreword to one of the first books on the Python programming language \citep{lutz:1996}:

\begin{quote}
Over six years ago, in December 1989, I was looking for a ``hobby'' programming project that would keep me occupied during the week around Christmas. My office ... would be closed, but I had a home computer, and not much else on my hands. I decided to write an interpreter for the new scripting language I had been thinking about lately: a descendant of ABC\footnote{ABC was a teaching language that van Rossum helped develop in the early eighties at \emph{Centrum   Wiskunde \& Informatica}. It was a language aimed at non-professional programmers.} that would appeal to Unix/C hackers. I chose Python as a working title for the project, being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus).
\end{quote}

The Python programming language, according to the nice definition that Wikipedia provides\footnote{\href{https://en.wikipedia.org/wiki/Python_(programming_language)}{https://en.wikipedia.org/wiki/Python\_(programming\_language)} accessed November 2016}, is a high-level, general-purpose, interpreted, dynamic programming language. Its design philosophy emphasizes code readability, and its syntax allows programmers to express concepts in fewer lines of code than possible in other widely used programming languages. Python provides constructs intended to enable writing clear programs on both a small and large scale.

It is necessary to distinguish between the specification of a programming language and its concrete implementation. A specification or technical standard is a set of grammatical, syntactic, and semantic rules and conventions that define how to write programs, and what those programs should do. A concrete implementation is what actual computers execute. There are several different implementations of the Python language, but the reference implementation ---that is the standard concrete form that implements the language specification--- is written in the C programming language and is named CPython to reflect this fact. The development of Python's reference implementation is lead by Guido van Rossum and has a community-based development model: a non-profit organization, the Python Software Foundation, acts as a legal umbrella to sponsor and direct the development of the Python language.

The analysis of the production process of Python presented in the following chapters focuses on the development of the CPython reference implementation of the Python programming language, but I'll refer to it as just the Python project henceforth for the sake of brevity.

The governance model of the Python project is based on public debates and discussions taking place in mailing lists and public meetings such as the language summits held each year in the annual Python conferences, where some Python developers meet face to face and discuss key issues in order to make decisions. The inventor of Python has however a lead role in settling disputes or arguments when the community of developers don't reach consensus. He has the power of making final decisions when there is no consensus. This is why he has the somewhat irreverent title of ``Benevolent Dictator For Life (BDFL)'' in the python community. 

In the beginning, the Python project started as an individual effort of Guido van Rossum, and has become one of the mainstream computer languages in the XXI century. Until 2000 it was almost an individual effort of van Rossum with few close collaborators. From 2000 the project gained popularity and several developers joined the project. In 2014, 172 individuals contributed at least one line of source code to Python project in all its history.

Nowadays, the Python language is widely used in several key areas of computing and software development. Some of the biggest websites of the World Wide Web (WWW) are powered by Python, such as youtube.com or reddit.com. Another main area where Python is very prominent is scientific computing and data processing and analysis. For instance, a big part of the data coming from the big telescopes on ---and around--- our planet are processed using tools written completely or partially in Python.

Finally, it is worth saying that most of the data processing, analysis, and graphical representations presented in this thesis are also written Python. The only other programming language used is the R language which is a programming language that focus on statistical computing.
